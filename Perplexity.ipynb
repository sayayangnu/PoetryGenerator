{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use:\n",
    "- Input 1: Corpus (Cleaned txt file)\n",
    "[1]\n",
    "- Input 2: The poem you generate, need to split to list of sentences (string)\n",
    "[12]\n",
    "- Output: \n",
    "- if only one sentence, use n-gram_perplexity functions\n",
    "- if multiple sentences, use total_n-gram_perplexity functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import + Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"corpus_CGR.txt\", encoding=\"utf-8\")\n",
    "corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "# corpus = re.sub(r'[^A-Za-z\\s\\']',\"\", corpus)\n",
    "# corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize original text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram List\n",
    "from nltk import ngrams\n",
    "unigramlist = tokens\n",
    "bigramlist = list(ngrams(tokens,2))\n",
    "trigramlist = list(ngrams(tokens,3))\n",
    "fourgramlist = list(ngrams(tokens,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram Frequency Dict\n",
    "unifreq = nltk.FreqDist(unigramlist)\n",
    "bifreq =  nltk.FreqDist(bigramlist)\n",
    "trifreq = nltk.FreqDist(trigramlist)\n",
    "fourfreq = nltk.FreqDist(fourgramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram MLE\n",
    "def unigram_mle(word): \n",
    "    if word in unigramlist:\n",
    "        return((unifreq[word]+1)/(len(unigramlist)+len(tokens)))\n",
    "    else:\n",
    "        return(1/(len(unigramlist)+len(tokens)))\n",
    "def bigram_mle(A,B):\n",
    "    if (B,A) in bigramlist :\n",
    "        return((1+bifreq[(B,A)])/(unifreq[B] + len(set(unifreq.keys()))))\n",
    "    else:\n",
    "        return(1/(unifreq[B]+ len(set(unifreq.keys()))))\n",
    "def trigram_mle(A,B,C):\n",
    "    if (B,C,A) in trigramlist :\n",
    "        return((1+trifreq[(B,C,A)])/(bifreq[(B,C)]+ len(set(unifreq.keys()))))    \n",
    "    else:\n",
    "        return((1)/(bifreq[(B,C)]+ len(set(unifreq.keys()))))\n",
    "\n",
    "def fourgram_mle(A,B,C,D): \n",
    "    if (B,C,D,A) in fourgramlist :\n",
    "        return((1+fourfreq[(B,C,D,A)])/(trifreq[(B,C,D)]+len(set(unifreq.keys()))))\n",
    "    else:\n",
    "        return((1)/(trifreq[(B,C,D)]+ len(set(trifreq.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Perplexities\n",
    "import math\n",
    "def unigram_perplexity(sentence):\n",
    "    words = sentence.split()\n",
    "    p = 1\n",
    "    for i in range(len(words)):\n",
    "        k = unigram_mle(words[i])\n",
    "        p = p*(1/k)**(1/len(words))\n",
    "    return(p)\n",
    "def total_unigram_perplexity(test):\n",
    "    k = 1\n",
    "    mo = 0\n",
    "    for i in test: \n",
    "        temp = unigram_perplexity(i)\n",
    "        k = k * (temp**(1/len(test)))\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Perplexity\n",
    "def bigram_perplexity(sentence):\n",
    "    words = sentence.split()\n",
    "    p = 1\n",
    "    for i in range(1,len(words)):\n",
    "        k = bigram_mle(words[i],words[i-1])\n",
    "        p = p*(1/k)**(1/len(words))\n",
    "    return(p)\n",
    "def total_bigram_perplexity(test):\n",
    "    k = 1\n",
    "    mo = 0\n",
    "    for i in test: \n",
    "        temp = bigram_perplexity(i)\n",
    "        k = k* (temp**(1/len(test)))\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram Perplexity\n",
    "def trigram_perplexity(sentence):\n",
    "    p = 1\n",
    "    words = sentence.split()\n",
    "    for i in range(2,len(words)):\n",
    "        k = trigram_mle(words[i],words[i-2],words[i-1])\n",
    "        p = p*(1/k)**(1/len(words))\n",
    "    return(p)\n",
    "def total_trigram_perplexity(test):\n",
    "    k = 1\n",
    "    for i in test: \n",
    "        temp = trigram_perplexity(i)\n",
    "        k = k* (temp**(1/len(test)))\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourgram Perplexity\n",
    "def fourgram_perplexity(sen):\n",
    "    p = 1\n",
    "    words = sen.split()\n",
    "    for i in range(3,len(words)):\n",
    "        k = fourgram_mle(words[i],words[i-3],words[i-2],words[i-1])\n",
    "        p = p*(1/k)**(1/len(words))\n",
    "    return(p)\n",
    "def total_fourgram_perplexity(test):\n",
    "    k = 1\n",
    "    for i in test: \n",
    "        temp = fourgram_perplexity(i)\n",
    "        k = k* (temp**(1/len(test)))\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple sentences, first split to list of strings\n",
    "text1 = [\"night was a wicked world having been tost\",\n",
    "         \"but a wicked world quake when one tailor el\"]\n",
    "text2 = [\n",
    "  \"You live his difficulty to the shore\",\n",
    "  \"Like a peace to frail other Eye\",\n",
    "  \"Desterial lamb out kindness so well\",\n",
    "  \"And they Peach back on field. Our home, who find it as his own\",\n",
    "  \"But where a grief cost thee they don't in youth\",\n",
    "\"And there is a bride in the flushing best night white\",\n",
    "  \"Where a gown upon the fortune land\",\n",
    "  \"Like an aspiring stroking gain, divine\", \"when maiden shall been sin\"\n",
    "  \"On her whirls for the orition of thine eyes\",\n",
    "  \"And was later and every mouth\",\n",
    "  \"It was my George each other's fear\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexities for Model 1 and 2 generated poems\n",
    "t11 = total_unigram_perplexity(text1)\n",
    "t12 = total_bigram_perplexity(text1)\n",
    "t13 = total_trigram_perplexity(text1)\n",
    "t14 = total_fourgram_perplexity(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t21 = total_unigram_perplexity(text2)\n",
    "t22 = total_bigram_perplexity(text2)\n",
    "t23 = total_trigram_perplexity(text2)\n",
    "t24 = total_fourgram_perplexity(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
