{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use:\n",
    "- Input 1: Corpus (Cleaned txt file)\n",
    "[1]\n",
    "- Input 2: The poem you generate, need to split to list of sentences (string)\n",
    "[12]\n",
    "- Output: \n",
    "- if only one sentence, use n-gram_perplexity functions\n",
    "- if multiple sentences, use total_n-gram_perplexity functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import + Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"corpus_CGR.txt\", encoding=\"utf-8\")\n",
    "corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "# corpus = re.sub(r'[^A-Za-z\\s\\']',\"\", corpus)\n",
    "# corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize original text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram List\n",
    "from nltk import ngrams\n",
    "unigramlist = tokens\n",
    "bigramlist = list(ngrams(tokens,2))\n",
    "trigramlist = list(ngrams(tokens,3))\n",
    "fourgramlist = list(ngrams(tokens,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram Frequency Dict\n",
    "unifreq = nltk.FreqDist(unigramlist)\n",
    "bifreq =  nltk.FreqDist(bigramlist)\n",
    "trifreq = nltk.FreqDist(trigramlist)\n",
    "fourfreq = nltk.FreqDist(fourgramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram MLE\n",
    "def unigram_mle(word): \n",
    "    if word in unigramlist:\n",
    "        return((unifreq[word]+1)/(len(unigramlist)+len(tokens)))\n",
    "    else:\n",
    "        return(1/(len(unigramlist)+len(tokens)))\n",
    "def bigram_mle(A,B):\n",
    "    if (B,A) in bigramlist :\n",
    "        return((1+bifreq[(B,A)])/(unifreq[B] + len(set(unifreq.keys()))))\n",
    "    else:\n",
    "        return(1/(unifreq[B]+ len(set(unifreq.keys()))))\n",
    "def trigram_mle(A,B,C):\n",
    "    if (B,C,A) in trigramlist :\n",
    "        return((1+trifreq[(B,C,A)])/(bifreq[(B,C)]+ len(set(unifreq.keys()))))    \n",
    "    else:\n",
    "        return((1)/(bifreq[(B,C)]+ len(set(unifreq.keys()))))\n",
    "\n",
    "def fourgram_mle(A,B,C,D): \n",
    "    if (B,C,D,A) in fourgramlist :\n",
    "        return((1+fourfreq[(B,C,D,A)])/(trifreq[(B,C,D)]+len(set(unifreq.keys()))))\n",
    "    else:\n",
    "        return((1)/(trifreq[(B,C,D)]+ len(set(trifreq.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Perplexities\n",
    "import math\n",
    "def unigram_perplexity(sentence):\n",
    "    words = sentence.split()\n",
    "    p = 1\n",
    "    for i in range(len(words)):\n",
    "        k = unigram_mle(words[i])\n",
    "        p = p*(1/k)**(1/len(words))\n",
    "    return(p)\n",
    "def total_unigram_perplexity(test):\n",
    "    k = 1\n",
    "    mo = 0\n",
    "    for i in test: \n",
    "        temp = unigram_perplexity(i)\n",
    "        k = k * (temp**(1/len(test)))\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Perplexity\n",
    "def bigram_perplexity(sentence):\n",
    "    words = sentence.split()\n",
    "    p = 1\n",
    "    for i in range(1,len(words)):\n",
    "        k = bigram_mle(words[i],words[i-1])\n",
    "        p = p*(1/k)**(1/len(words))\n",
    "    return(p)\n",
    "def total_bigram_perplexity(test):\n",
    "    k = 1\n",
    "    mo = 0\n",
    "    for i in test: \n",
    "        temp = bigram_perplexity(i)\n",
    "        k = k* (temp**(1/len(test)))\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram Perplexity\n",
    "def trigram_perplexity(sentence):\n",
    "    p = 1\n",
    "    words = sentence.split()\n",
    "    for i in range(2,len(words)):\n",
    "        k = trigram_mle(words[i],words[i-2],words[i-1])\n",
    "        p = p*(1/k)**(1/len(words))\n",
    "    return(p)\n",
    "def total_trigram_perplexity(test):\n",
    "    k = 1\n",
    "    for i in test: \n",
    "        temp = trigram_perplexity(i)\n",
    "        k = k* (temp**(1/len(test)))\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourgram Perplexity\n",
    "def fourgram_perplexity(sen):\n",
    "    p = 1\n",
    "    words = sen.split()\n",
    "    for i in range(3,len(words)):\n",
    "        k = fourgram_mle(words[i],words[i-3],words[i-2],words[i-1])\n",
    "        p = p*(1/k)**(1/len(words))\n",
    "    return(p)\n",
    "def total_fourgram_perplexity(test):\n",
    "    k = 1\n",
    "    for i in test: \n",
    "        temp = fourgram_perplexity(i)\n",
    "        k = k* (temp**(1/len(test)))\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple sentences, first split to list of strings\n",
    "text1 = [\"The flower at a wind who said the hanging albatross grapple on the sea sea couple man\"]\n",
    "text2 = [\"Love can't be described. key deadweed deadweed adorned harrow rook pollen pollen haul sores\"]\n",
    "text3 = [\n",
    "  \"That I stands his fingers o'er\",\n",
    "  \"Once as we will set off\",\n",
    "        \"To do then at its care\",\n",
    "    \"And can the deep wind course to say\",\n",
    "        \"Leastan\",\"greatest are both\",\n",
    "    \"Like some wonderful words the sake for life\",\n",
    "      \"But I know you bowed my world round down\",\n",
    "    \"So more than a something in the days\",\n",
    "      \"Bid me up and put on their grace\",\n",
    "    \"Looks\",\"What is gladly on his mile\",\n",
    "    \"With sertity drew real\",\n",
    "      \"The different clearly mere works fire\",\n",
    "    \"Sage from my heart lowth in the rain\",\n",
    "      \"To doth the cower\", \"and her new pleasures on my flame\",\n",
    "      \"Go din\", \"my care\"\n",
    "]\n",
    "text4 = [\"The wind is silent and the grave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram model of text1: 2140.202696993924\n",
      "Perplexity for Unigram model of text2: 175613.91093758805\n",
      "Perplexity for Unigram model of text3: 5069.772503608099\n",
      "Perplexity for Unigram model of text4: 603.3084449512252\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity for Unigram model of text1:\", total_unigram_perplexity(text1))\n",
    "print(\"Perplexity for Unigram model of text2:\", total_unigram_perplexity(text2))\n",
    "print(\"Perplexity for Unigram model of text3:\", total_unigram_perplexity(text3))\n",
    "print(\"Perplexity for Unigram model of text4:\", total_unigram_perplexity(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram model of text1: 3627.303744345597\n",
      "Perplexity for Bigram model of text2: 15298.700117670298\n",
      "Perplexity for Bigram model of text3: 546.7865057007787\n",
      "Perplexity for Bigram model of text4: 411.6899377591998\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity for Bigram model of text1:\", total_bigram_perplexity(text1))\n",
    "print(\"Perplexity for Bigram model of text2:\", total_bigram_perplexity(text2))\n",
    "print(\"Perplexity for Bigram model of text3:\", total_bigram_perplexity(text3))\n",
    "print(\"Perplexity for Bigram model of text4:\", total_bigram_perplexity(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Trigram model of text1: 7204.9616845944975\n",
      "Perplexity for Trigram model of text2: 7252.460250417535\n",
      "Perplexity for Trigram model of text3: 205.54940176402422\n",
      "Perplexity for Trigram model of text4: 1054.029697844953\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity for Trigram model of text1:\", total_trigram_perplexity(text1))\n",
    "print(\"Perplexity for Trigram model of text2:\", total_trigram_perplexity(text2))\n",
    "print(\"Perplexity for Trigram model of text3:\", total_trigram_perplexity(text3))\n",
    "print(\"Perplexity for Trigram model of text4:\", total_trigram_perplexity(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Fourgram model of text1: 51995.82281591697\n",
      "Perplexity for Fourgram model of text2: 31580.40489006827\n",
      "Perplexity for Fourgram model of text3: 165.65727519714898\n",
      "Perplexity for Fourgram model of text4: 1871.9925957309315\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity for Fourgram model of text1:\", total_fourgram_perplexity(text1))\n",
    "print(\"Perplexity for Fourgram model of text2:\", total_fourgram_perplexity(text2))\n",
    "print(\"Perplexity for Fourgram model of text3:\", total_fourgram_perplexity(text3))\n",
    "print(\"Perplexity for Fourgram model of text4:\", total_fourgram_perplexity(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
